{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67121ae0-3983-4f72-9972-b1b4ed8d1213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/2313606 files...\n",
      "Processed 1000/2313606 files...\n",
      "Processed 1500/2313606 files...\n",
      "Processed 2000/2313606 files...\n",
      "Processed 2500/2313606 files...\n",
      "Processed 3000/2313606 files...\n",
      "Processed 3500/2313606 files...\n",
      "Processed 4000/2313606 files...\n",
      "Processed 4500/2313606 files...\n",
      "Processed 5000/2313606 files...\n",
      "Processed 5500/2313606 files...\n",
      "Processed 6000/2313606 files...\n",
      "Processed 6500/2313606 files...\n",
      "Processed 7000/2313606 files...\n",
      "Processed 7500/2313606 files...\n",
      "Processed 8000/2313606 files...\n",
      "Processed 8500/2313606 files...\n",
      "Processed 9000/2313606 files...\n",
      "Processed 9500/2313606 files...\n",
      "Processed 10000/2313606 files...\n",
      "Processed 10500/2313606 files...\n",
      "Processed 11000/2313606 files...\n",
      "Processed 11500/2313606 files...\n",
      "Processed 12000/2313606 files...\n",
      "Processed 12500/2313606 files...\n",
      "Processed 13000/2313606 files...\n",
      "Processed 13500/2313606 files...\n",
      "Processed 14000/2313606 files...\n",
      "Processed 14500/2313606 files...\n",
      "Processed 15000/2313606 files...\n",
      "Processed 15500/2313606 files...\n",
      "Processed 16000/2313606 files...\n",
      "Processed 16500/2313606 files...\n",
      "Processed 17000/2313606 files...\n",
      "Processed 17500/2313606 files...\n",
      "Processed 18000/2313606 files...\n",
      "Processed 18500/2313606 files...\n",
      "Processed 19000/2313606 files...\n",
      "Processed 19500/2313606 files...\n",
      "Processed 20000/2313606 files...\n",
      "Processed 20500/2313606 files...\n",
      "Processed 21000/2313606 files...\n",
      "Processed 21500/2313606 files...\n",
      "Processed 22000/2313606 files...\n",
      "Processed 22500/2313606 files...\n",
      "Processed 23000/2313606 files...\n",
      "Processed 23500/2313606 files...\n",
      "Processed 24000/2313606 files...\n",
      "Processed 24500/2313606 files...\n",
      "Processed 25000/2313606 files...\n",
      "Processed 25500/2313606 files...\n",
      "Processed 26000/2313606 files...\n",
      "Processed 26500/2313606 files...\n",
      "Processed 27000/2313606 files...\n",
      "Processed 27500/2313606 files...\n",
      "Processed 28000/2313606 files...\n",
      "Processed 28500/2313606 files...\n",
      "Processed 29000/2313606 files...\n",
      "Processed 29500/2313606 files...\n",
      "Processed 30000/2313606 files...\n",
      "Processed 30500/2313606 files...\n",
      "Processed 31000/2313606 files...\n",
      "Processed 31500/2313606 files...\n",
      "Processed 32000/2313606 files...\n",
      "Processed 32500/2313606 files...\n",
      "Processed 33000/2313606 files...\n",
      "Processed 33500/2313606 files...\n",
      "Processed 34000/2313606 files...\n",
      "Processed 34500/2313606 files...\n",
      "Processed 35000/2313606 files...\n",
      "Processed 35500/2313606 files...\n",
      "Processed 36000/2313606 files...\n",
      "Processed 36500/2313606 files...\n",
      "Processed 37000/2313606 files...\n",
      "Processed 37500/2313606 files...\n",
      "Processed 38000/2313606 files...\n",
      "Processed 38500/2313606 files...\n",
      "Processed 39000/2313606 files...\n",
      "Processed 39500/2313606 files...\n",
      "Processed 40000/2313606 files...\n",
      "Processed 40500/2313606 files...\n",
      "Processed 41000/2313606 files...\n",
      "Processed 41500/2313606 files...\n",
      "Processed 42000/2313606 files...\n",
      "Processed 42500/2313606 files...\n",
      "Processed 43000/2313606 files...\n",
      "Processed 43500/2313606 files...\n",
      "Processed 44000/2313606 files...\n",
      "Processed 44500/2313606 files...\n",
      "Processed 45000/2313606 files...\n",
      "Processed 45500/2313606 files...\n",
      "Processed 46000/2313606 files...\n",
      "Processed 46500/2313606 files...\n",
      "Processed 47000/2313606 files...\n",
      "Processed 47500/2313606 files...\n",
      "Processed 48000/2313606 files...\n",
      "Processed 48500/2313606 files...\n",
      "Processed 49000/2313606 files...\n",
      "Processed 49500/2313606 files...\n",
      "Processed 50000/2313606 files...\n",
      "Processed 50500/2313606 files...\n",
      "Processed 51000/2313606 files...\n",
      "Processed 51500/2313606 files...\n",
      "Processed 52000/2313606 files...\n",
      "Processed 52500/2313606 files...\n",
      "Processed 53000/2313606 files...\n",
      "Processed 53500/2313606 files...\n",
      "Processed 54000/2313606 files...\n",
      "Processed 54500/2313606 files...\n",
      "Processed 55000/2313606 files...\n",
      "Processed 55500/2313606 files...\n",
      "Processed 56000/2313606 files...\n",
      "Processed 56500/2313606 files...\n",
      "Processed 57000/2313606 files...\n",
      "Processed 57500/2313606 files...\n",
      "Processed 58000/2313606 files...\n",
      "Processed 58500/2313606 files...\n",
      "Processed 59000/2313606 files...\n",
      "Processed 59500/2313606 files...\n",
      "Processed 60000/2313606 files...\n",
      "Processed 60500/2313606 files...\n",
      "Processed 61000/2313606 files...\n",
      "Processed 61500/2313606 files...\n",
      "Processed 62000/2313606 files...\n",
      "Processed 62500/2313606 files...\n",
      "Processed 63000/2313606 files...\n",
      "Processed 63500/2313606 files...\n",
      "Processed 64000/2313606 files...\n",
      "Processed 64500/2313606 files...\n",
      "Processed 65000/2313606 files...\n",
      "Processed 65500/2313606 files...\n",
      "Processed 66000/2313606 files...\n",
      "Processed 66500/2313606 files...\n",
      "Processed 67000/2313606 files...\n",
      "Processed 67500/2313606 files...\n",
      "Processed 68000/2313606 files...\n",
      "Processed 68500/2313606 files...\n",
      "Processed 69000/2313606 files...\n",
      "Processed 69500/2313606 files...\n",
      "Processed 70000/2313606 files...\n",
      "Processed 70500/2313606 files...\n",
      "Processed 71000/2313606 files...\n",
      "Processed 71500/2313606 files...\n",
      "Processed 72000/2313606 files...\n",
      "Processed 72500/2313606 files...\n",
      "Processed 73000/2313606 files...\n",
      "Processed 73500/2313606 files...\n",
      "Processed 74000/2313606 files...\n",
      "Processed 74500/2313606 files...\n",
      "Processed 75000/2313606 files...\n",
      "Processed 75500/2313606 files...\n",
      "Processed 76000/2313606 files...\n",
      "Processed 76500/2313606 files...\n",
      "Processed 77000/2313606 files...\n",
      "Processed 77500/2313606 files...\n",
      "Processed 78000/2313606 files...\n",
      "Processed 78500/2313606 files...\n",
      "Processed 79000/2313606 files...\n",
      "Processed 79500/2313606 files...\n",
      "Processed 80000/2313606 files...\n",
      "Processed 80500/2313606 files...\n",
      "Processed 81000/2313606 files...\n",
      "Processed 81500/2313606 files...\n",
      "Processed 82000/2313606 files...\n",
      "Processed 82500/2313606 files...\n",
      "Processed 83000/2313606 files...\n",
      "Processed 83500/2313606 files...\n",
      "Processed 84000/2313606 files...\n",
      "Processed 84500/2313606 files...\n",
      "Processed 85000/2313606 files...\n",
      "Processed 85500/2313606 files...\n",
      "Processed 86000/2313606 files...\n",
      "Processed 86500/2313606 files...\n",
      "Processed 87000/2313606 files...\n",
      "Processed 87500/2313606 files...\n",
      "Processed 88000/2313606 files...\n",
      "Processed 88500/2313606 files...\n",
      "Processed 89000/2313606 files...\n",
      "Processed 89500/2313606 files...\n",
      "Processed 90000/2313606 files...\n",
      "Processed 90500/2313606 files...\n",
      "Processed 91000/2313606 files...\n",
      "Processed 91500/2313606 files...\n",
      "Processed 92000/2313606 files...\n",
      "Processed 92500/2313606 files...\n",
      "Processed 93000/2313606 files...\n",
      "Processed 93500/2313606 files...\n",
      "Processed 94000/2313606 files...\n",
      "Processed 94500/2313606 files...\n",
      "Processed 95000/2313606 files...\n",
      "Processed 95500/2313606 files...\n",
      "Processed 96000/2313606 files...\n",
      "Processed 96500/2313606 files...\n",
      "Processed 97000/2313606 files...\n",
      "Processed 97500/2313606 files...\n",
      "Processed 98000/2313606 files...\n",
      "Processed 98500/2313606 files...\n",
      "Processed 99000/2313606 files...\n",
      "Processed 99500/2313606 files...\n",
      "Processed 100000/2313606 files...\n",
      "Processed 100500/2313606 files...\n",
      "Processed 101000/2313606 files...\n",
      "Processed 101500/2313606 files...\n",
      "Processed 102000/2313606 files...\n",
      "Processed 102500/2313606 files...\n",
      "Processed 103000/2313606 files...\n",
      "Processed 103500/2313606 files...\n",
      "Processed 104000/2313606 files...\n",
      "Processed 104500/2313606 files...\n",
      "Processed 105000/2313606 files...\n",
      "Processed 105500/2313606 files...\n",
      "Processed 106000/2313606 files...\n",
      "Processed 106500/2313606 files...\n",
      "Processed 107000/2313606 files...\n",
      "Processed 107500/2313606 files...\n",
      "Processed 108000/2313606 files...\n",
      "Processed 108500/2313606 files...\n",
      "Processed 109000/2313606 files...\n",
      "Processed 109500/2313606 files...\n",
      "Processed 110000/2313606 files...\n",
      "Processed 110500/2313606 files...\n",
      "Processed 111000/2313606 files...\n",
      "Processed 111500/2313606 files...\n",
      "Processed 112000/2313606 files...\n",
      "Processed 112500/2313606 files...\n",
      "Processed 113000/2313606 files...\n",
      "Processed 113500/2313606 files...\n",
      "Processed 114000/2313606 files...\n",
      "Processed 114500/2313606 files...\n",
      "Processed 115000/2313606 files...\n",
      "Processed 115500/2313606 files...\n",
      "Processed 116000/2313606 files...\n",
      "Processed 116500/2313606 files...\n",
      "Processed 117000/2313606 files...\n",
      "Processed 117500/2313606 files...\n",
      "Processed 118000/2313606 files...\n",
      "Processed 118500/2313606 files...\n",
      "Processed 119000/2313606 files...\n",
      "Processed 119500/2313606 files...\n",
      "Processed 120000/2313606 files...\n",
      "Processed 120500/2313606 files...\n",
      "Processed 121000/2313606 files...\n",
      "Processed 121500/2313606 files...\n",
      "Processed 122000/2313606 files...\n",
      "Processed 122500/2313606 files...\n",
      "Processed 123000/2313606 files...\n",
      "Processed 123500/2313606 files...\n",
      "Processed 124000/2313606 files...\n",
      "Processed 124500/2313606 files...\n",
      "Processed 125000/2313606 files...\n",
      "Processed 125500/2313606 files...\n",
      "Processed 126000/2313606 files...\n",
      "Processed 126500/2313606 files...\n",
      "Processed 127000/2313606 files...\n",
      "Processed 127500/2313606 files...\n",
      "Processed 128000/2313606 files...\n",
      "Processed 128500/2313606 files...\n",
      "Processed 129000/2313606 files...\n",
      "Processed 129500/2313606 files...\n",
      "Processed 130000/2313606 files...\n",
      "Processed 130500/2313606 files...\n",
      "Processed 131000/2313606 files...\n",
      "Processed 131500/2313606 files...\n",
      "Processed 132000/2313606 files...\n",
      "Processed 132500/2313606 files...\n",
      "Processed 133000/2313606 files...\n",
      "Processed 133500/2313606 files...\n",
      "Processed 134000/2313606 files...\n",
      "Processed 134500/2313606 files...\n",
      "Processed 135000/2313606 files...\n",
      "Processed 135500/2313606 files...\n",
      "Processed 136000/2313606 files...\n",
      "Processed 136500/2313606 files...\n",
      "Processed 137000/2313606 files...\n",
      "Processed 137500/2313606 files...\n",
      "Processed 138000/2313606 files...\n",
      "Processed 138500/2313606 files...\n",
      "Processed 139000/2313606 files...\n",
      "Processed 139500/2313606 files...\n",
      "Processed 140000/2313606 files...\n",
      "Processed 140500/2313606 files...\n",
      "Processed 141000/2313606 files...\n",
      "Processed 141500/2313606 files...\n",
      "Processed 142000/2313606 files...\n",
      "Processed 142500/2313606 files...\n",
      "Processed 143000/2313606 files...\n",
      "Processed 143500/2313606 files...\n",
      "Processed 144000/2313606 files...\n",
      "Processed 144500/2313606 files...\n",
      "Processed 145000/2313606 files...\n",
      "Processed 145500/2313606 files...\n",
      "Processed 146000/2313606 files...\n",
      "Processed 146500/2313606 files...\n",
      "Processed 147000/2313606 files...\n",
      "Processed 147500/2313606 files...\n",
      "Processed 148000/2313606 files...\n",
      "Processed 148500/2313606 files...\n",
      "Processed 149000/2313606 files...\n",
      "Processed 149500/2313606 files...\n",
      "Processed 150000/2313606 files...\n",
      "Processed 150500/2313606 files...\n",
      "Processed 151000/2313606 files...\n",
      "Processed 151500/2313606 files...\n",
      "Processed 152000/2313606 files...\n",
      "Processed 152500/2313606 files...\n",
      "Processed 153000/2313606 files...\n",
      "Processed 153500/2313606 files...\n",
      "Processed 154000/2313606 files...\n",
      "Processed 154500/2313606 files...\n",
      "Processed 155000/2313606 files...\n",
      "Processed 155500/2313606 files...\n",
      "Processed 156000/2313606 files...\n",
      "Processed 156500/2313606 files...\n",
      "Processed 157000/2313606 files...\n",
      "Processed 157500/2313606 files...\n",
      "Processed 158000/2313606 files...\n",
      "Processed 158500/2313606 files...\n",
      "Processed 159000/2313606 files...\n",
      "Processed 159500/2313606 files...\n",
      "Processed 160000/2313606 files...\n",
      "Processed 160500/2313606 files...\n",
      "Processed 161000/2313606 files...\n",
      "Processed 161500/2313606 files...\n",
      "Processed 162000/2313606 files...\n",
      "Processed 162500/2313606 files...\n",
      "Processed 163000/2313606 files...\n",
      "Processed 163500/2313606 files...\n",
      "Processed 164000/2313606 files...\n",
      "Processed 164500/2313606 files...\n",
      "Processed 165000/2313606 files...\n",
      "Processed 165500/2313606 files...\n",
      "Processed 166000/2313606 files...\n",
      "Processed 166500/2313606 files...\n",
      "Processed 167000/2313606 files...\n",
      "Processed 167500/2313606 files...\n",
      "Processed 168000/2313606 files...\n",
      "Processed 168500/2313606 files...\n",
      "Processed 169000/2313606 files...\n",
      "Processed 169500/2313606 files...\n",
      "Processed 170000/2313606 files...\n",
      "Processed 170500/2313606 files...\n",
      "Processed 171000/2313606 files...\n",
      "Processed 171500/2313606 files...\n",
      "Processed 172000/2313606 files...\n",
      "Processed 172500/2313606 files...\n",
      "Processed 173000/2313606 files...\n",
      "Processed 173500/2313606 files...\n",
      "Processed 174000/2313606 files...\n",
      "Processed 174500/2313606 files...\n",
      "Processed 175000/2313606 files...\n",
      "Processed 175500/2313606 files...\n",
      "Processed 176000/2313606 files...\n",
      "Processed 176500/2313606 files...\n",
      "Processed 177000/2313606 files...\n",
      "Processed 177500/2313606 files...\n",
      "Processed 178000/2313606 files...\n",
      "Processed 178500/2313606 files...\n",
      "Processed 179000/2313606 files...\n",
      "Processed 179500/2313606 files...\n",
      "Processed 180000/2313606 files...\n",
      "Processed 180500/2313606 files...\n",
      "Processed 181000/2313606 files...\n",
      "Processed 181500/2313606 files...\n",
      "Processed 182000/2313606 files...\n",
      "Processed 182500/2313606 files...\n",
      "Processed 183000/2313606 files...\n",
      "Processed 183500/2313606 files...\n",
      "Processed 184000/2313606 files...\n",
      "Processed 184500/2313606 files...\n",
      "Processed 185000/2313606 files...\n",
      "Processed 185500/2313606 files...\n",
      "Processed 186000/2313606 files...\n",
      "Processed 186500/2313606 files...\n",
      "Processed 187000/2313606 files...\n",
      "Processed 187500/2313606 files...\n",
      "Processed 188000/2313606 files...\n",
      "Processed 188500/2313606 files...\n",
      "Processed 189000/2313606 files...\n",
      "Processed 189500/2313606 files...\n",
      "Processed 190000/2313606 files...\n",
      "Processed 190500/2313606 files...\n",
      "Processed 191000/2313606 files...\n",
      "Processed 191500/2313606 files...\n",
      "Processed 192000/2313606 files...\n",
      "Processed 192500/2313606 files...\n",
      "Processed 193000/2313606 files...\n",
      "Processed 193500/2313606 files...\n",
      "Processed 194000/2313606 files...\n",
      "Processed 194500/2313606 files...\n",
      "Processed 195000/2313606 files...\n",
      "Processed 195500/2313606 files...\n",
      "Processed 196000/2313606 files...\n",
      "Processed 196500/2313606 files...\n",
      "Processed 197000/2313606 files...\n",
      "Processed 197500/2313606 files...\n",
      "Processed 198000/2313606 files...\n",
      "Processed 198500/2313606 files...\n",
      "Processed 199000/2313606 files...\n",
      "Processed 199500/2313606 files...\n",
      "Processed 200000/2313606 files...\n",
      "Processed 200500/2313606 files...\n",
      "Processed 201000/2313606 files...\n",
      "Processed 201500/2313606 files...\n",
      "Processed 202000/2313606 files...\n",
      "Processed 202500/2313606 files...\n",
      "Processed 203000/2313606 files...\n",
      "Processed 203500/2313606 files...\n",
      "Processed 204000/2313606 files...\n",
      "Processed 204500/2313606 files...\n",
      "Processed 205000/2313606 files...\n",
      "Processed 205500/2313606 files...\n",
      "Processed 206000/2313606 files...\n",
      "Processed 206500/2313606 files...\n",
      "Processed 207000/2313606 files...\n",
      "Processed 207500/2313606 files...\n",
      "Processed 208000/2313606 files...\n",
      "Processed 208500/2313606 files...\n",
      "Processed 209000/2313606 files...\n",
      "Processed 209500/2313606 files...\n",
      "Processed 210000/2313606 files...\n",
      "Processed 210500/2313606 files...\n",
      "Processed 211000/2313606 files...\n",
      "Processed 211500/2313606 files...\n",
      "Processed 212000/2313606 files...\n",
      "Processed 212500/2313606 files...\n",
      "Processed 213000/2313606 files...\n",
      "Processed 213500/2313606 files...\n",
      "Processed 214000/2313606 files...\n",
      "Processed 214500/2313606 files...\n",
      "Processed 215000/2313606 files...\n",
      "Processed 215500/2313606 files...\n",
      "Processed 216000/2313606 files...\n",
      "Processed 216500/2313606 files...\n",
      "Processed 217000/2313606 files...\n",
      "Processed 217500/2313606 files...\n",
      "Processed 218000/2313606 files...\n",
      "Processed 218500/2313606 files...\n",
      "Processed 219000/2313606 files...\n",
      "Processed 219500/2313606 files...\n",
      "Processed 220000/2313606 files...\n",
      "Processed 220500/2313606 files...\n",
      "Processed 221000/2313606 files...\n",
      "Processed 221500/2313606 files...\n",
      "Processed 222000/2313606 files...\n",
      "Processed 222500/2313606 files...\n",
      "Processed 223000/2313606 files...\n",
      "Processed 223500/2313606 files...\n",
      "Processed 224000/2313606 files...\n",
      "Processed 224500/2313606 files...\n",
      "Processed 225000/2313606 files...\n",
      "Processed 225500/2313606 files...\n",
      "Processed 226000/2313606 files...\n",
      "Processed 226500/2313606 files...\n",
      "Processed 227000/2313606 files...\n",
      "Processed 227500/2313606 files...\n",
      "Processed 228000/2313606 files...\n",
      "Processed 228500/2313606 files...\n",
      "Processed 229000/2313606 files...\n",
      "Processed 229500/2313606 files...\n",
      "Processed 230000/2313606 files...\n",
      "Processed 230500/2313606 files...\n",
      "Processed 231000/2313606 files...\n",
      "Processed 231500/2313606 files...\n",
      "Processed 232000/2313606 files...\n",
      "Processed 232500/2313606 files...\n",
      "Processed 233000/2313606 files...\n",
      "Processed 233500/2313606 files...\n",
      "Processed 234000/2313606 files...\n",
      "Processed 234500/2313606 files...\n",
      "Processed 235000/2313606 files...\n",
      "Processed 235500/2313606 files...\n",
      "Processed 236000/2313606 files...\n",
      "Processed 236500/2313606 files...\n",
      "Processed 237000/2313606 files...\n",
      "Processed 237500/2313606 files...\n",
      "Processed 238000/2313606 files...\n",
      "Processed 238500/2313606 files...\n",
      "Processed 239000/2313606 files...\n",
      "Processed 239500/2313606 files...\n",
      "Processed 240000/2313606 files...\n",
      "Processed 240500/2313606 files...\n",
      "Processed 241000/2313606 files...\n",
      "Processed 241500/2313606 files...\n",
      "Processed 242000/2313606 files...\n",
      "Processed 242500/2313606 files...\n",
      "Processed 243000/2313606 files...\n",
      "Processed 243500/2313606 files...\n",
      "Processed 244000/2313606 files...\n",
      "Processed 244500/2313606 files...\n",
      "Processed 245000/2313606 files...\n",
      "Processed 245500/2313606 files...\n",
      "Processed 246000/2313606 files...\n",
      "Processed 246500/2313606 files...\n",
      "Processed 247000/2313606 files...\n",
      "Processed 247500/2313606 files...\n",
      "Processed 248000/2313606 files...\n",
      "Processed 248500/2313606 files...\n",
      "Processed 249000/2313606 files...\n",
      "Processed 249500/2313606 files...\n",
      "Processed 250000/2313606 files...\n",
      "Processed 250500/2313606 files...\n",
      "Processed 251000/2313606 files...\n",
      "Processed 251500/2313606 files...\n",
      "Processed 252000/2313606 files...\n",
      "Processed 252500/2313606 files...\n",
      "Processed 253000/2313606 files...\n",
      "Processed 253500/2313606 files...\n",
      "Processed 254000/2313606 files...\n",
      "Processed 254500/2313606 files...\n",
      "Processed 255000/2313606 files...\n",
      "Processed 255500/2313606 files...\n",
      "Processed 256000/2313606 files...\n",
      "Processed 256500/2313606 files...\n",
      "Processed 257000/2313606 files...\n",
      "Processed 257500/2313606 files...\n",
      "Processed 258000/2313606 files...\n",
      "Processed 258500/2313606 files...\n",
      "Processed 259000/2313606 files...\n",
      "Processed 259500/2313606 files...\n",
      "Processed 260000/2313606 files...\n",
      "Processed 260500/2313606 files...\n",
      "Processed 261000/2313606 files...\n",
      "Processed 261500/2313606 files...\n",
      "Processed 262000/2313606 files...\n",
      "Processed 262500/2313606 files...\n",
      "Processed 263000/2313606 files...\n",
      "Processed 263500/2313606 files...\n",
      "Processed 264000/2313606 files...\n",
      "Processed 264500/2313606 files...\n",
      "Processed 265000/2313606 files...\n",
      "Processed 265500/2313606 files...\n",
      "Processed 266000/2313606 files...\n",
      "Processed 266500/2313606 files...\n",
      "Processed 267000/2313606 files...\n",
      "Processed 267500/2313606 files...\n",
      "Processed 268000/2313606 files...\n",
      "Processed 268500/2313606 files...\n",
      "Processed 269000/2313606 files...\n",
      "Processed 269500/2313606 files...\n",
      "Processed 270000/2313606 files...\n",
      "Processed 270500/2313606 files...\n",
      "Processed 271000/2313606 files...\n",
      "Processed 271500/2313606 files...\n",
      "Processed 272000/2313606 files...\n",
      "Processed 272500/2313606 files...\n",
      "Processed 273000/2313606 files...\n",
      "Processed 273500/2313606 files...\n",
      "Processed 274000/2313606 files...\n",
      "Processed 274500/2313606 files...\n",
      "Processed 275000/2313606 files...\n",
      "Processed 275500/2313606 files...\n",
      "Processed 276000/2313606 files...\n",
      "Processed 276500/2313606 files...\n",
      "Processed 277000/2313606 files...\n",
      "Processed 277500/2313606 files...\n",
      "Processed 278000/2313606 files...\n",
      "Processed 278500/2313606 files...\n",
      "Processed 279000/2313606 files...\n",
      "Processed 279500/2313606 files...\n",
      "Processed 280000/2313606 files...\n",
      "Processed 280500/2313606 files...\n",
      "Processed 281000/2313606 files...\n",
      "Processed 281500/2313606 files...\n",
      "Processed 282000/2313606 files...\n",
      "Processed 282500/2313606 files...\n",
      "Processed 283000/2313606 files...\n",
      "Processed 283500/2313606 files...\n",
      "Processed 284000/2313606 files...\n",
      "Processed 284500/2313606 files...\n",
      "Processed 285000/2313606 files...\n",
      "Processed 285500/2313606 files...\n",
      "Processed 286000/2313606 files...\n",
      "Processed 286500/2313606 files...\n",
      "Processed 287000/2313606 files...\n",
      "Processed 287500/2313606 files...\n",
      "Processed 288000/2313606 files...\n",
      "Processed 288500/2313606 files...\n",
      "Processed 289000/2313606 files...\n",
      "Processed 289500/2313606 files...\n",
      "Processed 290000/2313606 files...\n",
      "Processed 290500/2313606 files...\n",
      "Processed 291000/2313606 files...\n",
      "Processed 291500/2313606 files...\n",
      "Processed 292000/2313606 files...\n",
      "Processed 292500/2313606 files...\n",
      "Processed 293000/2313606 files...\n",
      "Processed 293500/2313606 files...\n",
      "Processed 294000/2313606 files...\n",
      "Processed 294500/2313606 files...\n",
      "Processed 295000/2313606 files...\n",
      "Processed 295500/2313606 files...\n",
      "Processed 296000/2313606 files...\n",
      "Processed 296500/2313606 files...\n",
      "Processed 297000/2313606 files...\n",
      "Processed 297500/2313606 files...\n",
      "Processed 298000/2313606 files...\n",
      "Processed 298500/2313606 files...\n",
      "Processed 299000/2313606 files...\n",
      "Processed 299500/2313606 files...\n",
      "Processed 300000/2313606 files...\n",
      "Processed 300500/2313606 files...\n",
      "Processed 301000/2313606 files...\n",
      "Processed 301500/2313606 files...\n",
      "Processed 302000/2313606 files...\n",
      "Processed 302500/2313606 files...\n",
      "Processed 303000/2313606 files...\n",
      "Processed 303500/2313606 files...\n",
      "Processed 304000/2313606 files...\n",
      "Processed 304500/2313606 files...\n",
      "Processed 305000/2313606 files...\n",
      "Processed 305500/2313606 files...\n",
      "Processed 306000/2313606 files...\n",
      "Processed 306500/2313606 files...\n",
      "Processed 307000/2313606 files...\n",
      "Processed 307500/2313606 files...\n",
      "Processed 308000/2313606 files...\n",
      "Processed 308500/2313606 files...\n",
      "Processed 309000/2313606 files...\n",
      "Processed 309500/2313606 files...\n",
      "Processed 310000/2313606 files...\n",
      "Processed 310500/2313606 files...\n",
      "Processed 311000/2313606 files...\n",
      "Processed 311500/2313606 files...\n",
      "Processed 312000/2313606 files...\n",
      "Processed 312500/2313606 files...\n",
      "Processed 313000/2313606 files...\n",
      "Processed 313500/2313606 files...\n",
      "Processed 314000/2313606 files...\n",
      "Processed 314500/2313606 files...\n",
      "Processed 315000/2313606 files...\n",
      "Processed 315500/2313606 files...\n",
      "Processed 316000/2313606 files...\n",
      "Processed 316500/2313606 files...\n",
      "Processed 317000/2313606 files...\n",
      "Processed 317500/2313606 files...\n",
      "Processed 318000/2313606 files...\n",
      "Processed 318500/2313606 files...\n",
      "Processed 319000/2313606 files...\n",
      "Processed 319500/2313606 files...\n",
      "Processed 320000/2313606 files...\n",
      "Processed 320500/2313606 files...\n",
      "Processed 321000/2313606 files...\n",
      "Processed 321500/2313606 files...\n",
      "Processed 322000/2313606 files...\n",
      "Processed 322500/2313606 files...\n",
      "Processed 323000/2313606 files...\n",
      "Processed 323500/2313606 files...\n",
      "Processed 324000/2313606 files...\n",
      "Processed 324500/2313606 files...\n",
      "Processed 325000/2313606 files...\n",
      "Processed 325500/2313606 files...\n",
      "Processed 326000/2313606 files...\n",
      "Processed 326500/2313606 files...\n",
      "Processed 327000/2313606 files...\n",
      "Processed 327500/2313606 files...\n",
      "Processed 328000/2313606 files...\n",
      "Processed 328500/2313606 files...\n",
      "Processed 329000/2313606 files...\n",
      "Processed 329500/2313606 files...\n",
      "Processed 330000/2313606 files...\n",
      "Processed 330500/2313606 files...\n",
      "Processed 331000/2313606 files...\n",
      "Processed 331500/2313606 files...\n",
      "Processed 332000/2313606 files...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 65\u001b[0m         workflow_data \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m         paths_and_values \u001b[38;5;241m=\u001b[39m extract_paths(workflow_data)\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# Add each path-value pair to the batch data\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/main.py:451\u001b[0m, in \u001b[0;36mYAML.load\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    449\u001b[0m constructor, parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_constructor_parser(stream)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     parser\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/constructor.py:114\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomposer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:72\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m document: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n\u001b[0;32m---> 72\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:94\u001b[0m, in \u001b[0;36mComposer.compose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:130\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m--> 130\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolver\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:211\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    206\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m#     raise ComposerError(\"while composing a mapping\",\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#             start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m#             \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    213\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:130\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m--> 130\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolver\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:211\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    206\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m#     raise ComposerError(\"while composing a mapping\",\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#             start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m#             \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    213\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:130\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m--> 130\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolver\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:211\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    206\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m#     raise ComposerError(\"while composing a mapping\",\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#             start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m#             \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    213\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:128\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    126\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_scalar_node(anchor)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(SequenceStartEvent):\n\u001b[0;32m--> 128\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_sequence_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[1;32m    130\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_mapping_node(anchor)\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:173\u001b[0m, in \u001b[0;36mComposer.compose_sequence_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    171\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(SequenceEndEvent):\n\u001b[0;32m--> 173\u001b[0m     node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    174\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    175\u001b[0m end_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:130\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m--> 130\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolver\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:211\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    206\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m#     raise ComposerError(\"while composing a mapping\",\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m#             start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m#             \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    213\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/composer.py:106\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompose_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent: Any, index: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAliasEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m         event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[1;32m    108\u001b[0m         alias \u001b[38;5;241m=\u001b[39m event\u001b[38;5;241m.\u001b[39manchor\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/parser.py:141\u001b[0m, in \u001b[0;36mParser.check_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/parser.py:631\u001b[0m, in \u001b[0;36mParser.parse_block_mapping_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscanner\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# value token might have post comment move it to e.g. block\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mValueToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_token_comment(token)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:170\u001b[0m, in \u001b[0;36mScanner.check_token\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mchoices: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_more_tokens():\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:294\u001b[0m, in \u001b[0;36mScanner.fetch_more_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Is it a literal scalar?\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_level:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_literal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Is it a folded scalar?\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_level:\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:680\u001b[0m, in \u001b[0;36mScanner.fetch_literal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_literal\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_block_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:691\u001b[0m, in \u001b[0;36mScanner.fetch_block_scalar\u001b[0;34m(self, style)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_possible_simple_key()\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# Scan and add SCALAR.\u001b[39;00m\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_block_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:1153\u001b[0m, in \u001b[0;36mScanner.scan_block_scalar\u001b[0;34m(self, style, rt)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_document_start() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_document_end():\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn \u001b[38;5;241m==\u001b[39m indent \u001b[38;5;129;01mand\u001b[39;00m srp() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1154\u001b[0m \n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# Unfortunately, folding rules are ambiguous.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# This is the folding according to the specification:\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rt \u001b[38;5;129;01mand\u001b[39;00m folded \u001b[38;5;129;01mand\u001b[39;00m line_break \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1160\u001b[0m         chunks\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\a\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PhD/Codes/github-workflows-analysis/myenv/lib/python3.13/site-packages/ruamel/yaml/scanner.py:148\u001b[0m, in \u001b[0;36mScanner.reader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml_version: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_directives: List[Tuple[Any, Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scanner_reader  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Initialize YAML parser with safe mode and duplicate key handling\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "yaml.allow_duplicate_keys = False\n",
    "\n",
    "# Function to extract paths and values from YAML data\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(item)))\n",
    "    return paths\n",
    "\n",
    "\n",
    "\n",
    "# Set the path to the workflows directory and output files\n",
    "extracted_path = '/Users/aref/Desktop/PhD/Datasets/workflows/workflows/'\n",
    "output_file = 'workflows_data.parquet'\n",
    "log_file = 'error_log.txt'\n",
    "\n",
    "# Initialize an empty DataFrame for storing results incrementally\n",
    "columns = ['workflow_id', 'path', 'value']\n",
    "batch_size = 500  # Process files in batches of this size\n",
    "\n",
    "# Clear previous log file if it exists\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Start processing files\n",
    "yaml_files = [f for f in os.listdir(extracted_path)]\n",
    "total_files = len(yaml_files)\n",
    "processed_files = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(0, total_files, batch_size):\n",
    "    batch_files = yaml_files[i:i + batch_size]\n",
    "    batch_data = []\n",
    "\n",
    "    for file in batch_files:\n",
    "        file_path = os.path.join(extracted_path, file)\n",
    "        workflow_id = file.split('.')[0]  # Assuming the file name is the workflow ID\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                workflow_data = yaml.load(f)\n",
    "                paths_and_values = extract_paths(workflow_data)\n",
    "\n",
    "                # Add each path-value pair to the batch data\n",
    "                for path, value in paths_and_values:\n",
    "                    batch_data.append({'workflow_id': workflow_id, 'path': path, 'value': value})\n",
    "        except Exception as e:\n",
    "            # Log errors with details about problematic files\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(f\"Error processing file {file}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "    # Convert batch data to DataFrame and append to parquet file incrementally\n",
    "    if batch_data:\n",
    "        df_batch = pd.DataFrame(batch_data, columns=columns)\n",
    "        \n",
    "        # Ensure all values in both 'path' and 'value' columns are strings\n",
    "        df_batch['path'] = df_batch['path'].astype(str)\n",
    "        df_batch['value'] = df_batch['value'].astype(str)\n",
    "        \n",
    "        # Fill NaN values with empty strings\n",
    "        df_batch = df_batch.fillna('')\n",
    "        \n",
    "        table = pa.Table.from_pandas(df_batch)\n",
    "\n",
    "    \n",
    "        if not os.path.exists(output_file):\n",
    "            pq.write_table(table, output_file)\n",
    "    else:\n",
    "        # Append to existing Parquet file\n",
    "        with pq.ParquetWriter(output_file, table.schema, use_dictionary=True) as writer:\n",
    "            writer.write_table(table)\n",
    "\n",
    "    processed_files += len(batch_files)\n",
    "    print(f\"Processed {processed_files}/{total_files} files...\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Processing completed!\")\n",
    "print(f\"Total number of workflows processed: {processed_files}\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Errors logged in: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc7fc98-a664-484a-98df-a625789474c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aref/Desktop/PhD/Codes/github-workflows-analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64754346-edec-475d-9fe4-66a6e77f0ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f17ee-2a2b-40e2-b050-42e7163b059a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454b21d-68eb-4a7d-b7e5-03a09d824aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e876ba-0807-4e03-8bc5-d31745c0151b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12812fbe-be86-4224-a979-351463c8a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea98c2-49c0-4385-81b1-ea535293b00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6387502-952f-46d4-be47-6a68e7492c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc2905-2751-4500-95b5-e8ee826bfd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3e1e26-3372-48fd-a482-ba6e6dab24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths and Values for 3d193364f960e446c7ead86ea75d537054a5dd25a5cc5467cbac1c066ed27cc1:\n",
      "name: darts PR merge workflow\n",
      "on.push.branches[0]: master\n",
      "jobs.lint.runs-on: ubuntu-latest\n",
      "jobs.lint.steps[0].name: 1. Clone repository\n",
      "jobs.lint.steps[0].uses: actions/checkout@v2\n",
      "jobs.lint.steps[1].name: 2. Set up Python 3.10\n",
      "jobs.lint.steps[1].uses: actions/setup-python@v1\n",
      "jobs.lint.steps[1].with.python-version: 3.10\n",
      "jobs.lint.steps[2].name: 3. Cache gradle distribution\n",
      "jobs.lint.steps[2].uses: actions/cache@v2\n",
      "jobs.lint.steps[2].with.path: ~/.gradle/wrapper/dists\n",
      "jobs.lint.steps[2].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties') }}\n",
      "jobs.lint.steps[3].name: 3.1 Cache gradle packages\n",
      "jobs.lint.steps[3].uses: actions/cache@v2\n",
      "jobs.lint.steps[3].with.path: ~/.gradle/caches\n",
      "jobs.lint.steps[3].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties', 'build.gradle') }}\n",
      "jobs.lint.steps[4].name: 4. Lint\n",
      "jobs.lint.steps[4].run: ./gradlew lint\n",
      "\n",
      "jobs.tests.runs-on: ${{ matrix.os }}\n",
      "jobs.tests.strategy.matrix.os[0]: macos-latest\n",
      "jobs.tests.strategy.matrix.os[1]: ubuntu-latest\n",
      "jobs.tests.strategy.matrix.python-version[0]: 3.8\n",
      "jobs.tests.strategy.matrix.python-version[1]: 3.10\n",
      "jobs.tests.strategy.matrix.flavour[0]: core\n",
      "jobs.tests.strategy.matrix.flavour[1]: torch\n",
      "jobs.tests.strategy.matrix.flavour[2]: all\n",
      "jobs.tests.steps[0].name: 1. Clone repository\n",
      "jobs.tests.steps[0].uses: actions/checkout@v2\n",
      "jobs.tests.steps[1].name: 2. Set up Python ${{ matrix.python-version }}\n",
      "jobs.tests.steps[1].uses: actions/setup-python@v1\n",
      "jobs.tests.steps[1].with.python-version: ${{ matrix.python-version }}\n",
      "jobs.tests.steps[2].name: 3. Cache gradle distribution\n",
      "jobs.tests.steps[2].uses: actions/cache@v2\n",
      "jobs.tests.steps[2].with.path: ~/.gradle/wrapper/dists\n",
      "jobs.tests.steps[2].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties') }}\n",
      "jobs.tests.steps[3].name: 3.1 Cache gradle packages\n",
      "jobs.tests.steps[3].uses: actions/cache@v2\n",
      "jobs.tests.steps[3].with.path: ~/.gradle/caches\n",
      "jobs.tests.steps[3].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties', 'build.gradle') }}\n",
      "jobs.tests.steps[4].name: 4. Setup pip\n",
      "jobs.tests.steps[4].run: ./gradlew setupPip\n",
      "\n",
      "jobs.tests.steps[5].name: 5. Install libomp (for LightGBM)\n",
      "jobs.tests.steps[5].run: ./.github/scripts/libomp-${{ runner.os }}.sh\n",
      "\n",
      "jobs.tests.steps[6].name: 6. Tests\n",
      "jobs.tests.steps[6].run: ./gradlew \"test_${{matrix.flavour}}\"\n",
      "\n",
      "jobs.tests.steps[7].name: 7. Codecov upload\n",
      "jobs.tests.steps[7].if: ${{ matrix.flavour == 'all' }}\n",
      "jobs.tests.steps[7].uses: codecov/codecov-action@v2\n",
      "jobs.tests.steps[7].with.fail_ci_if_error: True\n",
      "jobs.tests.steps[7].with.files: ./coverage.xml\n",
      "jobs.check-examples.runs-on: ubuntu-latest\n",
      "jobs.check-examples.strategy.matrix.example-name[0]: 00-quickstart.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[1]: 01-multi-time-series-and-covariates.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[2]: 02-data-processing.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[3]: 03-FFT-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[4]: 04-RNN-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[5]: 05-TCN-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[6]: 06-Transformer-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[7]: 07-NBEATS-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[8]: 08-DeepAR-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[9]: 09-DeepTCN-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[10]: 10-Kalman-filter-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[11]: 11-GP-filter-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[12]: 12-Dynamic-Time-Warping-example.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[13]: 13-TFT-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[14]: 15-static-covariates.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[15]: 16-hierarchical-reconciliation.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[16]: 18-TiDE-examples.ipynb\n",
      "jobs.check-examples.strategy.matrix.example-name[17]: 19-EnsembleModel-examples.ipynb\n",
      "jobs.check-examples.steps[0].name: 1. Clone repository\n",
      "jobs.check-examples.steps[0].uses: actions/checkout@v2\n",
      "jobs.check-examples.steps[1].name: 2. Set up Python 3.9\n",
      "jobs.check-examples.steps[1].uses: actions/setup-python@v1\n",
      "jobs.check-examples.steps[1].with.python-version: 3.9\n",
      "jobs.check-examples.steps[2].name: 3. Cache gradle distribution\n",
      "jobs.check-examples.steps[2].uses: actions/cache@v2\n",
      "jobs.check-examples.steps[2].with.path: ~/.gradle/wrapper/dists\n",
      "jobs.check-examples.steps[2].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties') }}\n",
      "jobs.check-examples.steps[3].name: 3.1 Cache gradle packages\n",
      "jobs.check-examples.steps[3].uses: actions/cache@v2\n",
      "jobs.check-examples.steps[3].with.path: ~/.gradle/caches\n",
      "jobs.check-examples.steps[3].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties', 'build.gradle') }}\n",
      "jobs.check-examples.steps[4].name: 4. Run examples ${{matrix.example-name}}\n",
      "jobs.check-examples.steps[4].run: ./gradlew checkExample -PexampleName=${{matrix.example-name}}\n",
      "\n",
      "jobs.docs.runs-on: ubuntu-latest\n",
      "jobs.docs.steps[0].name: 1. Clone repository\n",
      "jobs.docs.steps[0].uses: actions/checkout@v2\n",
      "jobs.docs.steps[1].name: 2. Set up Python 3.9\n",
      "jobs.docs.steps[1].uses: actions/setup-python@v1\n",
      "jobs.docs.steps[1].with.python-version: 3.9\n",
      "jobs.docs.steps[2].name: 3. Install pandoc\n",
      "jobs.docs.steps[2].run: sudo apt-get install -y pandoc\n",
      "\n",
      "jobs.docs.steps[3].name: 4. Cache gradle distribution\n",
      "jobs.docs.steps[3].uses: actions/cache@v2\n",
      "jobs.docs.steps[3].with.path: ~/.gradle/wrapper/dists\n",
      "jobs.docs.steps[3].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties') }}\n",
      "jobs.docs.steps[4].name: 4.1 Cache gradle packages\n",
      "jobs.docs.steps[4].uses: actions/cache@v2\n",
      "jobs.docs.steps[4].with.path: ~/.gradle/caches\n",
      "jobs.docs.steps[4].with.key: tests-${{ runner.os }}-gradle-${{ hashFiles('gradle/wrapper/gradle-wrapper.properties', 'build.gradle') }}\n",
      "jobs.docs.steps[5].name: 5. Setup pip\n",
      "jobs.docs.steps[5].run: ./gradlew setupPip\n",
      "\n",
      "jobs.docs.steps[6].name: 6. Attach cache for pip\n",
      "jobs.docs.steps[6].uses: actions/cache@v1\n",
      "jobs.docs.steps[6].id: cache\n",
      "jobs.docs.steps[6].with.path: ~/.cache/pip\n",
      "jobs.docs.steps[6].with.key: release-${{ runner.os }}-3.9-pip-${{ hashFiles('requirements/core.txt', 'requirements/release.txt') }}\n",
      "jobs.docs.steps[7].name: 7. Build docs\n",
      "jobs.docs.steps[7].run: ./gradlew buildDocs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "    # Function to extract paths and values from YAML data\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(item)))\n",
    "    return paths\n",
    "\n",
    "    \n",
    "\n",
    "# Set the path to the workflows directory\n",
    "extracted_path = '/Users/aref/Desktop/PhD/Datasets/workflows/workflows/'\n",
    "\n",
    "# Get the first YAML file in the directory\n",
    "yaml_files = [f for f in os.listdir(extracted_path)]\n",
    "if not yaml_files:\n",
    "    print(\"No YAML files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "first_file = yaml_files[0]\n",
    "file_path = os.path.join(extracted_path, first_file)\n",
    "\n",
    "# Parse the YAML file\n",
    "yaml = YAML()\n",
    "with open(file_path, 'r') as file:\n",
    "    workflow_data = yaml.load(file)\n",
    "\n",
    "# Extract paths and values\n",
    "paths_and_values = extract_paths(workflow_data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Paths and Values for {first_file}:\")\n",
    "for path, value in paths_and_values:\n",
    "    print(f\"{path}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9445a-0f00-444d-9856-1776e3a79a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adac3d-75e2-4e5b-bb93-6b44d7af63f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
