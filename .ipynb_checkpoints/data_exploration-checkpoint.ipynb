{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Initialize YAML parser with safe mode and duplicate key handling\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "yaml.allow_duplicate_keys = False\n",
    "\n",
    "# Function to extract paths and values from YAML data\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(item)))\n",
    "    return paths\n",
    "\n",
    "\n",
    "\n",
    "# Set the path to the workflows directory and output files\n",
    "extracted_path = '/Users/aref/Desktop/PhD/Datasets/workflows/workflows/'\n",
    "output_file = 'workflows_data.parquet'\n",
    "log_file = 'error_log.txt'\n",
    "\n",
    "# Initialize an empty DataFrame for storing results incrementally\n",
    "columns = ['workflow_id', 'path', 'value']\n",
    "batch_size = 500  # Process files in batches of this size\n",
    "\n",
    "# Clear previous log file if it exists\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Start processing files\n",
    "yaml_files = [f for f in os.listdir(extracted_path)]\n",
    "total_files = len(yaml_files)\n",
    "processed_files = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(0, total_files, batch_size):\n",
    "    batch_files = yaml_files[i:i + batch_size]\n",
    "    batch_data = []\n",
    "\n",
    "    for file in batch_files:\n",
    "        file_path = os.path.join(extracted_path, file)\n",
    "        workflow_id = file.split('.')[0]  # Assuming the file name is the workflow ID\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                workflow_data = yaml.load(f)\n",
    "                paths_and_values = extract_paths(workflow_data)\n",
    "\n",
    "                # Add each path-value pair to the batch data\n",
    "                for path, value in paths_and_values:\n",
    "                    batch_data.append({'workflow_id': workflow_id, 'path': path, 'value': value})\n",
    "        except Exception as e:\n",
    "            # Log errors with details about problematic files\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(f\"Error processing file {file}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "    # Convert batch data to DataFrame and append to parquet file incrementally\n",
    "    if batch_data:\n",
    "        df_batch = pd.DataFrame(batch_data, columns=columns)\n",
    "        \n",
    "        # Ensure all values in both 'path' and 'value' columns are strings\n",
    "        df_batch['path'] = df_batch['path'].astype(str)\n",
    "        df_batch['value'] = df_batch['value'].astype(str)\n",
    "        \n",
    "        # Fill NaN values with empty strings\n",
    "        df_batch = df_batch.fillna('')\n",
    "        \n",
    "        table = pa.Table.from_pandas(df_batch)\n",
    "\n",
    "    \n",
    "        if not os.path.exists(output_file):\n",
    "            pq.write_table(table, output_file)\n",
    "    else:\n",
    "        # Append to existing Parquet file\n",
    "        with pq.ParquetWriter(output_file, table.schema, use_dictionary=True) as writer:\n",
    "            writer.write_table(table)\n",
    "\n",
    "    processed_files += len(batch_files)\n",
    "    print(f\"Processed {processed_files}/{total_files} files...\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Processing completed!\")\n",
    "print(f\"Total number of workflows processed: {processed_files}\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Errors logged in: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimenting: getting number of distinct workflows ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('workflows_data.parquet')\n",
    "\n",
    "# Group by 'workflow_id' and count unique workflows\n",
    "unique_workflows = df['workflow_id'].nunique()\n",
    "\n",
    "# Get total number of rows and columns\n",
    "total_rows = len(df)\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "# Display the information\n",
    "print(f\"Total number of unique workflows: {unique_workflows}\")\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "\n",
    "# If you want to see the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "    # Function to extract paths and values from YAML data\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(item)))\n",
    "    return paths\n",
    "\n",
    "    \n",
    "\n",
    "# Set the path to the workflows directory\n",
    "extracted_path = '/Users/aref/Desktop/PhD/Datasets/workflows/workflows/'\n",
    "\n",
    "# Get the first YAML file in the directory\n",
    "yaml_files = [f for f in os.listdir(extracted_path)]\n",
    "if not yaml_files:\n",
    "    print(\"No YAML files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "first_file = yaml_files[0]\n",
    "file_path = os.path.join(extracted_path, first_file)\n",
    "\n",
    "# Parse the YAML file\n",
    "yaml = YAML()\n",
    "with open(file_path, 'r') as file:\n",
    "    workflow_data = yaml.load(file)\n",
    "\n",
    "# Extract paths and values\n",
    "paths_and_values = extract_paths(workflow_data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Paths and Values for {first_file}:\")\n",
    "for path, value in paths_and_values:\n",
    "    print(f\"{path}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
