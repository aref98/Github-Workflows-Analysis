{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Lets have look at workflow metadata databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path if necessary\n",
    "metadata_path = '../../data-raw/Workflows-Metadata/workflows.csv.gz'\n",
    "\n",
    "# Read the first few lines of the compressed CSV\n",
    "df_workflows = pd.read_csv(metadata_path, compression='gzip')\n",
    "print(len(df_workflows))\n",
    "df_workflows.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of workflows: {len(df_workflows)} \")\n",
    "print (f\"Unique number of workflows: {df_workflows['uid'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows = df_workflows[(df_workflows[\"file_hash\"] == df_workflows[\"previous_file_hash\"]) & (df_workflows[\"file_path\"] == df_workflows[\"previous_file_path\"] )]\n",
    "\n",
    "matching_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now its time filter valid workflow files. Since we want to only extract path/value pairs from those their validness checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path if necessary\n",
    "metadata_path = './data-raw/Workflows-Metadata/workflows.csv.gz'\n",
    "output_valid_hashes='./data/valid_workflow_hashes.parquet'\n",
    "# Load the metadata dataset\n",
    "df_workflows = pd.read_csv(metadata_path, compression='gzip')\n",
    "\n",
    "# Filter only the rows where the workflow is valid\n",
    "df_valid_workflows = df_workflows[df_workflows['valid_workflow'] == True] \n",
    "\n",
    "# Extract the unique file hashes of valid workflows\n",
    "valid_workflow_hashes = df_valid_workflows['file_hash'].unique()\n",
    "\n",
    "df_valid_hashes = pd.DataFrame(valid_workflow_hashes, columns=['file_hash'])\n",
    "df_valid_hashes.to_parquet(output_valid_hashes, index=False)\n",
    "\n",
    "# At this point, we have an array of file_hashes that correspond to valid workflows.\n",
    "\n",
    "\n",
    "print(f\"Number of valid workflows: {len(valid_workflow_hashes)} out of: {len(df_workflows)} workflows\")\n",
    "print(\"Example of valid workflow hashes:\", valid_workflow_hashes[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    " We can then use `valid_workflow_hashes` in the next steps to:\n",
    " 1. Load only those workflows from Workflow Files.\n",
    " 2. Extract (path, value) pairs from them.\n",
    " 3. Store the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now for this cell we will do these three steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "\n",
    "# Paths and configurations\n",
    "extracted_path = '../data-raw/Workflow-Files/workflows'\n",
    "valid_hashes_parquet = '../data/valid_workflow_hashes.parquet'\n",
    "output_file = '../data/workflows_data.parquet'\n",
    "log_file = 'error_log.txt'\n",
    "\n",
    "batch_size = 500\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "yaml.allow_duplicate_keys = False\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Load valid workflow hashes\n",
    "df_valid_hashes = pd.read_parquet(valid_hashes_parquet)\n",
    "valid_hashes = set(df_valid_hashes['file_hash'])\n",
    "\n",
    "# Get list of YAML files\n",
    "all_files = os.listdir(extracted_path)\n",
    "yaml_files = [f for f in all_files if f in valid_hashes]\n",
    "total_files = len(yaml_files)\n",
    "\n",
    "# Check for existing log file and remove it\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Helper functions\n",
    "def check_for_on_key(data):\n",
    "    if \"on\" in data:\n",
    "        w_on = data[\"on\"]\n",
    "        if not isinstance(w_on, dict):\n",
    "            if not isinstance(w_on, list):\n",
    "                w_on = [w_on]\n",
    "            data[\"on\"] = {k: None for k in w_on}\n",
    "    return data\n",
    "\n",
    "def extract_paths(data, prefix=None):\n",
    "    \"\"\"\n",
    "    Recursively extract the \"path\" of each leaf node as a list of keys,\n",
    "    and the corresponding value of that leaf node.\n",
    "    \"\"\"\n",
    "    if prefix is None:\n",
    "        prefix = []  # start empty\n",
    "\n",
    "    results = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = prefix + [key]\n",
    "            if isinstance(value, (dict, list)):\n",
    "                results.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                results.append((new_prefix, value))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = prefix + [str(index)]\n",
    "            if isinstance(item, (dict, list)):\n",
    "                results.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                results.append((new_prefix, item))\n",
    "    return results\n",
    "\n",
    "def process_file(file):\n",
    "    file_path = os.path.join(extracted_path, file)\n",
    "    workflow_hash = file\n",
    "    batch_data = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            workflow_data = yaml.load(f)\n",
    "        workflow_data = check_for_on_key(workflow_data)\n",
    "\n",
    "        paths_and_values = extract_paths(workflow_data)\n",
    "        for path, value in paths_and_values:\n",
    "            # Ensure each path is a proper list\n",
    "            if not isinstance(path, list):\n",
    "                path = [str(path)]  # Coerce to a list of strings\n",
    "            batch_data.append({\n",
    "                'workflow_hash': workflow_hash,\n",
    "                'path': path,\n",
    "                'value': value\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file}: {str(e)}\")\n",
    "    return batch_data\n",
    "\n",
    "# Initialize an empty DataFrame to accumulate data\n",
    "accumulated_data = []\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(0, total_files, batch_size):\n",
    "    batch_files = yaml_files[i: i + batch_size]\n",
    "    batch_data = []\n",
    "\n",
    "    # Process each file in the batch\n",
    "    for file in batch_files:\n",
    "        file_data = process_file(file)\n",
    "        batch_data.extend(file_data)\n",
    "\n",
    "    if batch_data:\n",
    "        df_batch = pd.DataFrame(batch_data)\n",
    "        df_batch['workflow_hash'] = df_batch['workflow_hash'].astype(str)\n",
    "        df_batch['value'] = df_batch['value'].astype(str)\n",
    "        \n",
    "        # Accumulate batch data\n",
    "        accumulated_data.append(df_batch)\n",
    "\n",
    "    print(f\"Processed {min(i + batch_size, total_files)}/{total_files} files...\")\n",
    "\n",
    "# Concatenate all accumulated data into a single DataFrame\n",
    "final_df = pd.concat(accumulated_data, ignore_index=True)\n",
    "\n",
    "# Ensure `path` is a list of strings for every row\n",
    "final_df['path'] = final_df['path'].apply(lambda x: [str(elem) for elem in x] if isinstance(x, list) else [])\n",
    "\n",
    "# Define the schema for PyArrow\n",
    "schema = pa.schema([\n",
    "    ('workflow_hash', pa.string()),\n",
    "    ('path', pa.list_(pa.string())),  # Explicitly define path as a list of strings\n",
    "    ('value', pa.string()),\n",
    "])\n",
    "\n",
    "# Convert the DataFrame to a PyArrow Table with the schema\n",
    "table = pa.Table.from_pandas(final_df, schema=schema)\n",
    "\n",
    "# Write the table to a Parquet file\n",
    "pq.write_table(table, output_file)\n",
    "\n",
    "print(f\"Processing completed! Data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Experimenting: getting number of distinct workflows (to check we successessfully process all the workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimenting: getting number of distinct workflows ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('../../data/workflows_data.parquet')\n",
    "\n",
    "# Group by 'workflow_id' and count unique workflows\n",
    "unique_workflows = df['workflow_hash'].nunique()\n",
    "\n",
    "# Get total number of rows and columns\n",
    "total_rows = len(df)\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "# Display the information\n",
    "print(f\"Total number of unique workflows: {unique_workflows}\")\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('../../data/workflows_data.parquet')\n",
    "\n",
    "# Convert any string 'None' values in the 'value' column to np.nan\n",
    "# df['value'] = df['value'].replace('None', None)\n",
    "\n",
    "# Filter the DataFrame for rows where the 'path' starts with 'on.'\n",
    "on_key_df = df[df['path'].apply(lambda path_list: path_list[0] == 'on')]\n",
    "\n",
    "# Identify rows where the 'value' is not in dictionary format or is None\n",
    "incorrect_values_df = on_key_df[(on_key_df['value'].isnull()) | \n",
    "                                (on_key_df['value'].apply(lambda x: isinstance(x, str) and x.lower() == 'none'))]\n",
    "\n",
    "# Display the results\n",
    "incorrect_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_values_df[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimenting: getting number of distinct workflows ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet files\n",
    "df = pd.read_parquet('../../data/workflows_data.parquet')\n",
    "\n",
    "# Group by 'workflow_id' and count unique workflows\n",
    "unique_workflows = df['workflow_hash'].nunique()\n",
    "\n",
    "# Get total number of rows and columns\n",
    "total_rows = len(df)\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "# Display the information\n",
    "print(f\"Total number of unique workflows: {unique_workflows}\")\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and returning the head of data from parquet file containing our path/value pairs\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "No we have a databse of workflows path/value pairs which each row is for a path/value pair of a workflow. As we can see each workflow got rows as many as its path/value pairs.\n",
    "\n",
    "Its time to enrich the database from more features for our evolution analysis. Since we want to study size, complexity, and use of concepts, we need more features (columns) from the metadata database. So we did a join to have an enriched database. we can adjust our columns in columns_to_keep list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # Paths to the databases\n",
    "# path_value_file = '../../data/workflows_data.parquet'\n",
    "# metadata_file = '../../data-raw/Workflows-Metadata/workflows.csv.gz'\n",
    "\n",
    "# # Load the metadata file\n",
    "# df_metadata = pd.read_csv(metadata_file, compression='gzip')\n",
    "\n",
    "# # Select columns to keep from metadata\n",
    "# columns_to_keep = [\n",
    "#     'repository', 'commit_hash', 'committed_date',\n",
    "#     'file_hash', 'previous_file_hash', 'uid'\n",
    "# ]\n",
    "# df_metadata = df_metadata[columns_to_keep]\n",
    "\n",
    "# # Read parquet file in smaller batches\n",
    "# parquet_file = pq.ParquetFile(path_value_file)\n",
    "# batch_size = 100000  \n",
    "\n",
    "# df_enhanced_list = []\n",
    "\n",
    "# for batch in parquet_file.iter_batches(batch_size):\n",
    "#     df_chunk = batch.to_pandas()\n",
    "#     df_chunk_enhanced = df_chunk.merge(\n",
    "#         df_metadata,\n",
    "#         left_on='workflow_hash',\n",
    "#         right_on='file_hash',\n",
    "#         how='inner'\n",
    "#     )\n",
    "#     df_enhanced_list.append(df_chunk_enhanced)\n",
    "\n",
    "# # Concatenate all chunks into a single DataFrame\n",
    "# df_enhanced = pd.concat(df_enhanced_list, ignore_index=True)\n",
    "\n",
    "# # Save the enhanced dataset\n",
    "# enhanced_output_file = './data/enhanced_workflows_data.parquet'\n",
    "# # df_enhanced.to_parquet(enhanced_output_file, index=False, engine='pyarrow')\n",
    "\n",
    "# # Display summary\n",
    "# print(f\"Enhanced dataset created with {len(df_enhanced)} rows.\")\n",
    "# df_enhanced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_file_hashes = df_metadata[df_metadata.duplicated('file_hash', keep=False)]\n",
    "print(f\"Number of duplicate file_hashes: {len(duplicate_file_hashes)}\")\n",
    "duplicate_file_hashes.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In the next cell, we will detect which concepts are used in a given workflow path (if any).\n",
    "\n",
    "Concepts List:\n",
    "\n",
    "Matrix Strategy,\n",
    "Permissions,\n",
    "Triggers,\n",
    "Reusable Actions,\n",
    "Reusable Workflows,\n",
    "Secrets,\n",
    "Environment Variables,\n",
    "Conditional Statements,\n",
    "Job Dependencies,\n",
    "\n",
    "These concepts are identified in the detect_concept function using predefined regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read the parquet file\n",
    "file_path = '../../data/workflows_data.parquet'\n",
    "output_file_path = '../../data/workflows_path_value_concepts.parquet'\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "DELIM = r\"::\"\n",
    "\n",
    "# IDENT = letters, digits, underscore, dot, hyphen, slash, colon,\n",
    "# dollar sign, question mark, plus spaces. \n",
    "\n",
    "IDENT = r\"(?:[^:]|:(?!:))+\"\n",
    "\n",
    "\n",
    "DIGIT = r\"\\d+\"\n",
    "STEP_INDEX = rf\"steps(?:{DELIM}{DIGIT})?\"\n",
    "INDEX = rf\"{DELIM}{DIGIT}\" \n",
    "\n",
    "\n",
    "patterns = [\n",
    "    # 1) TRIGGER (global level)\n",
    "    (\n",
    "        \"trigger\",\n",
    "        rf\"^on{DELIM}{IDENT}(?:{DELIM}{IDENT}(?:{INDEX})?)*$\"\n",
    "    ),\n",
    "    # 2) PERMISSIONS (global & job-level)\n",
    "    (\n",
    "        \"permissions_global\",\n",
    "        # Global: can be just [permissions] or [permissions, something, ...]\n",
    "        rf\"^permissions(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "    (\n",
    "        \"permissions_job_level\",\n",
    "        # Job-level: jobs::X::permissions(...) \n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}permissions(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 3) ENVIRONMENT VARIABLE (global, job, or step level)\n",
    "    (\n",
    "        \"environment_variable_global\",\n",
    "        # Global env: env or env::KEY1::KEY2...\n",
    "        rf\"^env(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "    (\n",
    "        \"environment_variable_job_level\",\n",
    "        # Job-level: jobs::X::env(...)\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}env(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "    (\n",
    "        \"environment_variable_step_level\",\n",
    "        # Step-level: jobs::X::steps(::\\d+)?::env(...)\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}env(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 4) WITH USAGE (job-level & step-level)\n",
    "    (\n",
    "        \"with_usage_job_level\",\n",
    "        # Job-level: jobs::X::with(...)\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}with(?:{DELIM}{IDENT})*$\",\n",
    "    ),\n",
    "    (\n",
    "        \"with_usage_step_level\",\n",
    "        # Step-level: jobs::X::steps(::\\d+)?::with(...)\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}with(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "\n",
    "    # 13) ENTRYPOINT (step-level & container-level)\n",
    "    (\n",
    "        \"entrypoint_step_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}with{DELIM}entrypoint$\"\n",
    "    ),\n",
    "    (\n",
    "        \"entrypoint_container_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}container{DELIM}entrypoint$\"\n",
    "    ),\n",
    "\n",
    "\n",
    "    # 5) ARGS (step-level)\n",
    "    (\n",
    "        \"args\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}with{DELIM}args$\"\n",
    "    ),\n",
    "\n",
    "    # 6) CONDITIONAL STATEMENT (job-level or step-level)\n",
    "    (\n",
    "        \"conditional_statement\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}if$\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}if$\"\n",
    "    ),\n",
    "\n",
    "    # 7) REUSABLE ACTION (step-level)\n",
    "    (\n",
    "        \"reusable_action\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}uses$\"\n",
    "    ),\n",
    "\n",
    "    # 8) REUSABLE WORKFLOW (job-level)\n",
    "    (\n",
    "        \"reusable_workflow\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}uses$\"\n",
    "    ),\n",
    "\n",
    "    # 9) MATRIX STRATEGY (job-level)\n",
    "    (\n",
    "        \"matrix_strategy\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}strategy{DELIM}matrix(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 10) MATRIX INCLUDE (job-level)\n",
    "    (\n",
    "        \"matrix_include\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}strategy{DELIM}matrix{DELIM}include(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 11) MATRIX EXCLUDE (job-level)\n",
    "    (\n",
    "        \"matrix_exclude\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}strategy{DELIM}matrix{DELIM}exclude(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 12) MAX PARALLEL (job-level)\n",
    "    (\n",
    "        \"max_parallel\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}strategy{DELIM}max-parallel$\"\n",
    "    ),\n",
    "\n",
    "    # 14) WORKING DIRECTORY (job-level defaults or step-level) \n",
    "    (\n",
    "        \"working_directory_job-level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}defaults{DELIM}run{DELIM}working-directory$\"\n",
    "    ),\n",
    "    (\n",
    "        \"working_directory_step-level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}working-directory$\"\n",
    "    ),\n",
    " \n",
    "\n",
    "    # 15) CONCURRENCY (global & job-level)\n",
    "    (\n",
    "        \"concurrency_global\",\n",
    "        # Global: concurrency or concurrency::something\n",
    "        rf\"^concurrency(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "    (\n",
    "        \"concurrency_job_level\",\n",
    "        # Job-level: jobs::X::concurrency(...)\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}concurrency(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 16) JOB OUTPUTS (job-level)\n",
    "    (\n",
    "        \"job_outputs\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}outputs(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 17) TIMEOUTS (job-level or step-level)\n",
    "    (\n",
    "        \"timeouts_job_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}timeout-minutes$\"\n",
    "    ),\n",
    "    (\n",
    "        \"timeouts_step_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}timeout-minutes$\"\n",
    "    ),\n",
    "\n",
    "    # 18) ERROR HANDLING (fail-fast or continue-on-error)\n",
    "    (\n",
    "        \"error_handling_fail_fast\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}strategy{DELIM}fail-fast$\"\n",
    "    ),\n",
    "    (\n",
    "        \"error_handling_continue_on_error\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}continue-on-error$\"\n",
    "    ),\n",
    "\n",
    "    # 19) CONTAINER (job-level)\n",
    "    (\n",
    "        \"container\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}container(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 20) CONTAINER OPTIONS (job-level)\n",
    "    (\n",
    "        \"container_options\",\n",
    "        # If you need multiple sub-tokens after 'options', use (?:{DELIM}{IDENT})* instead\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}container{DELIM}options(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 21) SERVICES (job-level)\n",
    "    (\n",
    "        \"services\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}services{DELIM}{IDENT}(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 22) SERVICES OPTIONS (job-level)\n",
    "    (\n",
    "        \"services_options\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}services{DELIM}{IDENT}{DELIM}options(?:{DELIM}{IDENT})*$\"\n",
    "    ),\n",
    "\n",
    "    # 23) COMMANDS (step-level)\n",
    "    (\n",
    "        \"commands\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}run$\"\n",
    "    ),\n",
    "\n",
    "    # 24) DEFAULTS (global & job-level)\n",
    "    (\n",
    "        \"defaults_global\",\n",
    "        rf\"^defaults(?:{DELIM}{IDENT}(?:{INDEX})?)*$\",\n",
    "    ),\n",
    "    (\n",
    "        \"defaults_job_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}defaults(?:{DELIM}{IDENT}(?:{INDEX})?)*$\"\n",
    "    ),\n",
    "\n",
    "    # 25) SHELL (global level)\n",
    "    (\n",
    "        \"shell_global\",\n",
    "        rf\"^defaults{DELIM}run{DELIM}shell$\"\n",
    "    ),\n",
    "    (\n",
    "        \"shell_job_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}defaults{DELIM}run{DELIM}shell$\"\n",
    "    ),\n",
    "    (\n",
    "        \"shell_step_level\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}{STEP_INDEX}{DELIM}shell$\"\n",
    "    ),\n",
    "\n",
    "\n",
    "    # 27) NEEDS (job-level)\n",
    "    (\n",
    "        \"needs\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}needs(?:{INDEX})?$\"\n",
    "    ),\n",
    "\n",
    "    # 28) RUNS-ON (job-level)\n",
    "    (\n",
    "        \"runs_on\",\n",
    "        rf\"^jobs{DELIM}{IDENT}{DELIM}runs-on(?:{DELIM}{IDENT})*$\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def check_presence(path):\n",
    "    \"\"\"\n",
    "    Join the path list with '::' and run regex checks against the joined path.\n",
    "    Return a dict of booleans for each concept.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {concept: False for concept, *_ in patterns}\n",
    "    for concept, *regex_list in patterns:\n",
    "        for regex in regex_list:\n",
    "            if re.search(regex, \"::\".join(path)):\n",
    "                results[concept] = True\n",
    "                break\n",
    "    return results\n",
    "\n",
    "\n",
    "table = pq.read_table(file_path, columns=[\"workflow_hash\", \"path\", \"value\"])\n",
    "df_len = table.num_rows\n",
    "CHUNK_SIZE = 50_000\n",
    "n_chunks = math.ceil(df_len / CHUNK_SIZE)\n",
    "\n",
    "start_time = time.time()\n",
    "first_write = True  # Track if we are writing the first chunk\n",
    "\n",
    "# Initialize ParquetWriter\n",
    "writer = None\n",
    "\n",
    "for i in tqdm(range(n_chunks), desc=\"Processing chunks\"):\n",
    "    start_idx = i * CHUNK_SIZE\n",
    "    end_idx = min((i + 1) * CHUNK_SIZE, df_len)\n",
    "    \n",
    "    # Convert slice of the table into a pandas DataFrame\n",
    "    df_chunk = table.slice(start_idx, end_idx - start_idx).to_pandas()\n",
    "\n",
    "    # Run concept checks chunk by chunk\n",
    "    presence_data = [check_presence(path) for path in df_chunk[\"path\"]]\n",
    "    presence_df = pd.DataFrame(presence_data)\n",
    "\n",
    "    # Append concept columns to df_chunk\n",
    "    for col in presence_df.columns:\n",
    "        df_chunk[col] = presence_df[col]\n",
    "\n",
    "    # Convert to PyArrow Table\n",
    "    table_result = pa.Table.from_pandas(df_chunk)\n",
    "\n",
    "    # Write to Parquet file\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_file_path, table_result.schema)\n",
    "    writer.write_table(table_result)\n",
    "\n",
    "    # Free memory manually\n",
    "    del df_chunk, presence_data, presence_df, table_result\n",
    "\n",
    "# Close the writer\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Done. Processed {df_len} rows in {elapsed:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_value_concepts_file_path = '../../data/workflows_path_value_features.parquet'\n",
    "\n",
    "df=pd.read_parquet(path_value_concepts_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
