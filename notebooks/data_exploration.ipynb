{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path if necessary\n",
    "metadata_path = './data-raw/Workflows-Metadata/workflows.csv.gz'\n",
    "\n",
    "# Read the first few lines of the compressed CSV\n",
    "df_workflows = pd.read_csv(metadata_path, compression='gzip')\n",
    "df_workflows.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of workflows: {len(df_workflows)} \")\n",
    "print (f\"Unique number of workflows: {df_workflows['uid'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path if necessary\n",
    "metadata_path = './data-raw/Workflows-Metadata/workflows.csv.gz'\n",
    "output_valid_hashes='./data/valid_workflow_hashes.parquet'\n",
    "# Load the metadata dataset\n",
    "df_workflows = pd.read_csv(metadata_path, compression='gzip')\n",
    "\n",
    "# Filter only the rows where the workflow is valid\n",
    "df_valid_workflows = df_workflows[df_workflows['valid_workflow'] == True] \n",
    "\n",
    "# Extract the unique file hashes of valid workflows\n",
    "valid_workflow_hashes = df_valid_workflows['file_hash'].unique()\n",
    "\n",
    "df_valid_hashes = pd.DataFrame(valid_workflow_hashes, columns=['file_hash'])\n",
    "df_valid_hashes.to_parquet(output_valid_hashes, index=False)\n",
    "\n",
    "# At this point, we have an array of file_hashes that correspond to valid workflows.\n",
    "\n",
    "\n",
    "print(f\"Number of valid workflows: {len(valid_workflow_hashes)} out of: {len(df_workflows)} workflows\")\n",
    "print(\"Example of valid workflow hashes:\", valid_workflow_hashes[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    " We can then use `valid_workflow_hashes` in the next steps to:\n",
    " 1. Load only those workflows from Workflow Files.\n",
    " 2. Extract (path, value) pairs from them.\n",
    " 3. Store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Path to the directory containing workflow files\n",
    "extracted_path = './data-raw/Workflow-Files/workflows'\n",
    "\n",
    "# Path to Parquet file containing valid workflow hashes (single column: 'file_hash')\n",
    "valid_hashes_parquet = './data/valid_workflow_hashes.parquet'\n",
    "\n",
    "# Output paths\n",
    "output_file = './data/workflows_data.parquet'\n",
    "log_file = 'error_log.txt'\n",
    "\n",
    "# Batch size for file processing\n",
    "batch_size = 500\n",
    "\n",
    "# Columns for the final output table\n",
    "columns = ['workflow_hash', 'path', 'value']\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. LOAD VALID WORKFLOW HASHES\n",
    "# ------------------------------------------------------------------------------\n",
    "df_valid_hashes = pd.read_parquet(valid_hashes_parquet)  # contains one column: 'file_hash'\n",
    "valid_hashes = set(df_valid_hashes['file_hash'])  # convert to a set for quick membership lookup\n",
    "print(f\"{len(valid_hashes)} valid workflow files\")\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. YAML PARSER INITIALIZATION\n",
    "# ------------------------------------------------------------------------------\n",
    "yaml = YAML(typ='safe', pure=True)\n",
    "yaml.allow_duplicate_keys = False\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. FUNCTION TO RECURSIVELY EXTRACT (PATH, VALUE) PAIRS (Based on DFS algorithm)\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((str(new_prefix), str(item)))\n",
    "    return paths\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. FILTER AND PROCESS ONLY VALID WORKFLOW FILES\n",
    "# ------------------------------------------------------------------------------\n",
    "# Since filenames are exactly the workflow_hash (no extensions), we can simply filter by membership in valid_hashes.\n",
    "all_files = os.listdir(extracted_path)\n",
    "yaml_files = [f for f in all_files if f in valid_hashes]\n",
    "\n",
    "\n",
    "total_files = len(yaml_files) \n",
    "processed_files = 0\n",
    "\n",
    "if (total_files==0):\n",
    "    print(\"No Valid Workflow has been found!\")\n",
    "\n",
    "# Remove previous log file if it exists\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# Initialize a DataFrame to store all path/value pairs \n",
    "all_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, total_files, batch_size):\n",
    "    batch_files = yaml_files[i : i + batch_size]\n",
    "    batch_data = []\n",
    "\n",
    "    for file in batch_files:\n",
    "        file_path = os.path.join(extracted_path, file)\n",
    "        \n",
    "        # In this case, 'file' is the workflow hash\n",
    "        workflow_hash = file\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                workflow_data = yaml.load(f)\n",
    "\n",
    "            paths_and_values = extract_paths(workflow_data)\n",
    "            \n",
    "            # Add each path-value pair to the batch data\n",
    "            for path, value in paths_and_values:\n",
    "                batch_data.append({\n",
    "                    'workflow_hash': workflow_hash,\n",
    "                    'path': path,\n",
    "                    'value': value\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log errors with details about problematic files\n",
    "            with open(log_file, 'a') as log:\n",
    "                log.write(f\"Error processing file {file}: {str(e)}\\n\")\n",
    "                print(f\"Error processing file {file}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "    # Convert batch data to DataFrame and write/append to Parquet\n",
    "    if batch_data:\n",
    "        df_batch = pd.DataFrame(batch_data, columns=columns)\n",
    "\n",
    "        # Ensure all 'path' and 'value' columns are strings\n",
    "        df_batch['path'] = df_batch['path'].astype(str)\n",
    "        df_batch['value'] = df_batch['value'].astype(str)\n",
    "\n",
    "        # Fill NaN values with empty strings (defensive measure)\n",
    "        df_batch = df_batch.fillna('')\n",
    "        all_data = pd.concat([all_data, df_batch])\n",
    "        \n",
    "\n",
    "    processed_files += len(batch_files)\n",
    "    print(f\"Processed {processed_files}/{total_files} valid workflow files...\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. CONVERT all_data TO PARQUET FILE AFTER PROCESSING\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "all_data_table = pa.Table.from_pandas(all_data)\n",
    "final_output_file = './data/workflows_data.parquet'\n",
    "\n",
    "# Write the entire all_data DataFrame to a new Parquet file\n",
    "pq.write_table(all_data_table, final_output_file)\n",
    "\n",
    "# Check number of unique workflows in the new Parquet file\n",
    "df_final_check = pd.read_parquet(final_output_file)\n",
    "unique_workflows_final_check = df_final_check['workflow_hash'].nunique()\n",
    "print(f\"Total number of unique workflows in final Parquet: {unique_workflows_final_check}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Processing completed!\")\n",
    "print(f\"Total valid workflow files processed: {processed_files}\")\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Errors logged in: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['workflow_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimenting: getting number of distinct workflows ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('./data/workflows_data.parquet')\n",
    "\n",
    "# Group by 'workflow_id' and count unique workflows\n",
    "unique_workflows = df['workflow_hash'].nunique()\n",
    "\n",
    "# Get total number of rows and columns\n",
    "total_rows = len(df)\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "# Display the information\n",
    "print(f\"Total number of unique workflows: {unique_workflows}\")\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "print(f\"Total number of columns: {total_columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and returning the head of data from parquet file containing our path/value pairs\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('./data/workflows_data.parquet')\n",
    "\n",
    "# Check for duplicate workflow hashes\n",
    "duplicate_hashes = df['workflow_hash'].duplicated(keep=False)\n",
    "duplicates = df[duplicate_hashes]\n",
    "print(f\"Number of duplicate rows: {len(duplicates)}\")\n",
    "print(duplicates['workflow_hash'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "    # Function to extract paths and values from YAML data\n",
    "def extract_paths(data, prefix=''):\n",
    "    paths = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                paths.extend(extract_paths(value, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(value)))\n",
    "    elif isinstance(data, list):\n",
    "        for index, item in enumerate(data):\n",
    "            new_prefix = f\"{prefix}[{index}]\"\n",
    "            if isinstance(item, (dict, list)):\n",
    "                paths.extend(extract_paths(item, new_prefix))\n",
    "            else:\n",
    "                paths.append((new_prefix, str(item)))\n",
    "    return paths\n",
    "\n",
    "    \n",
    "\n",
    "# Set the path to the workflows directory\n",
    "extracted_path = '/Users/aref/Desktop/PhD/Codes/github-workflows-analysis/test_workflows/'\n",
    "\n",
    "# Get the first YAML file in the directory\n",
    "yaml_files = [f for f in os.listdir(extracted_path)]\n",
    "if not yaml_files:\n",
    "    print(\"No YAML files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "first_file = yaml_files[1]\n",
    "file_path = os.path.join(extracted_path, first_file)\n",
    "\n",
    "# Parse the YAML file\n",
    "yaml = YAML()\n",
    "with open(file_path, 'r') as file:\n",
    "    workflow_data = yaml.load(file)\n",
    "\n",
    "# Extract paths and values\n",
    "paths_and_values = extract_paths(workflow_data)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Paths and Values for {first_file}:\")\n",
    "for path, value in paths_and_values:\n",
    "    print(f\"{path}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
